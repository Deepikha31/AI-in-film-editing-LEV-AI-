{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1a13116-910e-4387-81ad-1ca49ded2296",
      "metadata": {},
      "source": [
        "## Problem Definition & Objective\n",
        "\n",
        "### Problem Statement: \n",
        "Manual film editing is time-consuming and often ignores the psychological **\"Kuleshov Effect\"** where the meaning of a shot changes based on the context of the shots surrounding it.\n",
        "\n",
        "### Objective: \n",
        "To algorithmically construct emotionally expressive video montages using rule-based cinematic logic, demonstrating how AI-assisted tools can augment creative storytelling rather than replace it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b552229-391c-486e-a602-734f1e4d9a0e",
      "metadata": {},
      "source": [
        "## Selected Project Track: AI in Film Editing \n",
        "\n",
        "This project lies at the intersection of:\n",
        "- Artificial Intelligence\n",
        "- Computational Creativity\n",
        "- Film & Media Technology\n",
        "\n",
        "It focuses on AI-assisted creative systems rather than predictive modeling, emphasizing design logic, emotional representation, and responsible automation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0e5d62-ed72-4a3a-9d42-39f36f46015c",
      "metadata": {},
      "source": [
        "## Real-world Relevance: \n",
        "This tool allows creators to rapidly prototype emotional subtexts, using heuristic AI to automate complex editorial structures like \"Anxious\" pulsing or \"Flashback\" erasures.\n",
        "\n",
        "Emotion-driven editing is widely used in:\n",
        "- Film trailers\n",
        "- Mental health storytelling\n",
        "- Artistic short films\n",
        "- Social impact media\n",
        "\n",
        "Independent creators often lack access to advanced post-production tools. This system demonstrates how lightweight AI logic can help creators express complex emotional states programmatically, using accessible libraries and transparent logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb670a59-c3de-48f1-bd41-1e7c6ca8b9e3",
      "metadata": {},
      "source": [
        "## Data Understanding & Preparation\n",
        "\n",
        "### Input Data\n",
        "- Three short video clips representing narrative fragments\n",
        "- An user-selected emotional state (`mood`) either by choosing from a dropdown list or manual commands.\n",
        "\n",
        "### Assumptions\n",
        "- All clips share compatible resolution or are composited\n",
        "- Emotion is represented symbolically via editing rhythm, pacing, and repetition\n",
        "- No facial recognition or biometric data is used"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31eaac8a-4feb-4588-a95e-23991b21b8ea",
      "metadata": {},
      "source": [
        "## Libraries Used\n",
        "\n",
        "- Python 3.x\n",
        "- MoviePy\n",
        "- NumPy\n",
        "- FFmpeg (backend)\n",
        "\n",
        "This notebook assumes all dependencies are installed locally."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f97863f-2cee-4313-9126-373427ccefb7",
      "metadata": {},
      "source": [
        "## Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc344af-facb-45d3-8114-a3436c0ae9d3",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "\n",
        "from moviepy.editor import (\n",
        "    VideoFileClip,\n",
        "    concatenate_videoclips,\n",
        "    ColorClip\n",
        ")\n",
        "\n",
        "import moviepy.video.fx.all as vfx\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from IPython.display import Video"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fcdd9cc-e4d2-4876-a683-04ef88a5b0e7",
      "metadata": {},
      "source": [
        "## Model / System Design"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f6e1b0a-de65-4b26-a605-240ef94bbda9",
      "metadata": {},
      "source": [
        "This system follows a **rule-based emotional montage architecture**:\n",
        "\n",
        "- **Anxious** → Rapid interruptions with black frames (visual pulse)\n",
        "- **Flashback** → Forward playback followed by accelerated reverse\n",
        "- **Neutral** → Smooth linear continuity\n",
        "\n",
        "The \"model\" here is not a neural network but a **deterministic creative system**, where cinematic rules encode emotional intent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f7cfa3-4b08-42c6-9021-7361783a99a9",
      "metadata": {},
      "source": [
        "## Logic "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36213b62-9d6c-4caf-a975-4567a86b960c",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cinematic_montage.mp4'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===============================\n",
        "# Configuration (replaces UI)\n",
        "# ===============================\n",
        "\n",
        "mood = \"Anxious\"  \n",
        "# Options: Peaceful | Anxious | Flashback | Haze | None\n",
        "\n",
        "manual_enabled = (mood == \"None\")\n",
        "p_in = \"\"  # example: \"bw all reverse\"\n",
        "\n",
        "output_name = f\"montage_{int(time.time())}.mp4\"\n",
        "\n",
        "# ===============================\n",
        "# Load Clips (local files)\n",
        "# ===============================\n",
        "\n",
        "v1 = VideoFileClip(\"a.mp4\")\n",
        "v2 = VideoFileClip(\"b.mp4\")\n",
        "v3 = VideoFileClip(\"c.mp4\")\n",
        "\n",
        "# ===============================\n",
        "# Effect Functions\n",
        "# ===============================\n",
        "\n",
        "def haze_math(get_frame, t):\n",
        "    frame = get_frame(t)\n",
        "    blurred = np.zeros_like(frame)\n",
        "    for i in range(3):\n",
        "        blurred[:, :, i] = gaussian_filter(frame[:, :, i], sigma=8)\n",
        "    return blurred.astype(\"uint8\")\n",
        "\n",
        "def grayscale_math(get_frame, t):\n",
        "    frame = get_frame(t)\n",
        "    gray = np.dot(frame[..., :3], [0.299, 0.587, 0.114])\n",
        "    return np.stack([gray]*3, axis=-1).astype(\"uint8\")\n",
        "\n",
        "# ===============================\n",
        "# Clip Processing Logic\n",
        "# ===============================\n",
        "\n",
        "def process(clip, tag):\n",
        "    if mood == \"Haze\":\n",
        "        clip = clip.transform(haze_math)\n",
        "\n",
        "    if manual_enabled and p_in:\n",
        "        p = p_in.lower()\n",
        "        if tag in p or \"all\" in p:\n",
        "            if \"bw\" in p:\n",
        "                clip = clip.transform(grayscale_math)\n",
        "            if \"reverse\" in p:\n",
        "                clip = clip.with_effects([vfx.TimeMirror()])\n",
        "\n",
        "    return clip\n",
        "\n",
        "s1 = process(v1, \"first\")\n",
        "s2 = process(v2, \"second\")\n",
        "s3 = process(v3, \"third\")\n",
        "\n",
        "# ===============================\n",
        "# Assembly Logic\n",
        "# ===============================\n",
        "\n",
        "if mood == \"Anxious\":\n",
        "    black = ColorClip(size=v1.size, color=(0, 0, 0), duration=0.3)\n",
        "    content = concatenate_videoclips(\n",
        "        [s1, black, s2, black, s3, black],\n",
        "        method=\"compose\"\n",
        "    )\n",
        "\n",
        "elif mood == \"Flashback\":\n",
        "    fwd = concatenate_videoclips([s1, s2, s3], method=\"compose\")\n",
        "    rev = fwd.with_effects([\n",
        "        vfx.TimeMirror(),\n",
        "        vfx.MultiplySpeed(4.0)\n",
        "    ])\n",
        "    content = concatenate_videoclips([fwd, rev], method=\"compose\")\n",
        "\n",
        "else:\n",
        "    # Peaceful, Haze, None\n",
        "    content = concatenate_videoclips(\n",
        "        [s1, s2, s3],\n",
        "        method=\"compose\",\n",
        "        padding=-0.5\n",
        "    )\n",
        "\n",
        "# ===============================\n",
        "# Export & Display\n",
        "# ===============================\n",
        "\n",
        "content.write_videofile(\n",
        "    output_name,\n",
        "    codec=\"libx264\",\n",
        "    audio_codec=\"aac\",\n",
        "    fps=24\n",
        ")\n",
        "\n",
        "v1.close(); v2.close(); v3.close(); content.close()\n",
        "\n",
        "Video(output_name, embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f8d1c28",
      "metadata": {},
      "source": [
        "## Requirements \n",
        "moviepy\n",
        "numpy\n",
        "scipy\n",
        "imageio-ffmpeg\n",
        "ipykernel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e77884-2edb-4870-bf3d-979bdf232092",
      "metadata": {},
      "source": [
        "## Evaluation & Analysis\n",
        "\n",
        "### Qualitative Evaluation\n",
        "- Emotional intent is communicated through pacing and repetition\n",
        "- Anxiety is reinforced via visual interruption\n",
        "- Flashbacks simulate memory distortion\n",
        "\n",
        "### Limitations\n",
        "- Emotion selection is manual\n",
        "- No learning or personalization\n",
        "- Limited clip diversity\n",
        "\n",
        "Despite this, the system succeeds as a proof-of-concept for emotionally aware creative automation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95102c0f-0c41-44d7-8f03-12985fd5f062",
      "metadata": {},
      "source": [
        "## Ethical Considerations & Responsible AI\n",
        "\n",
        "- No personal, biometric, or sensitive data is used\n",
        "- The system avoids emotion inference or psychological diagnosis\n",
        "- Emotions are artist-defined, not user-profiled\n",
        "- Creative control remains fully with the human creator\n",
        "\n",
        "This aligns with Responsible AI principles of transparency, agency, and non-exploitation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184ddf69-e762-4f36-8677-265512f48a01",
      "metadata": {},
      "source": [
        "## Conclusion & Future Scope\n",
        "\n",
        "This project demonstrates how AI-assisted systems can support emotional storytelling without replacing human creativity.\n",
        "\n",
        "### Future Enhancements\n",
        "- Emotion detection from audio/music\n",
        "- ML-based pacing optimization\n",
        "- Integration with mobile filmmaking tools\n",
        "- Expanded emotional vocabulary\n",
        "\n",
        "The system positions AI as a creative collaborator, not an author.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.13 (XPython)",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
