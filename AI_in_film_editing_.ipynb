{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c1a13116-910e-4387-81ad-1ca49ded2296",
      "cell_type": "markdown",
      "source": "## Problem Definition & Objective\n\n### Problem Statement: \nManual film editing is time-consuming and often ignores the psychological **\"Kuleshov Effect\"** where the meaning of a shot changes based on the context of the shots surrounding it.\n\n### Objective: \nTo algorithmically construct emotionally expressive video montages using rule-based cinematic logic, demonstrating how AI-assisted tools can augment creative storytelling rather than replace it.",
      "metadata": {}
    },
    {
      "id": "0b552229-391c-486e-a602-734f1e4d9a0e",
      "cell_type": "markdown",
      "source": "## Selected Project Track: AI in Film Editing \n\nThis project lies at the intersection of:\n- Artificial Intelligence\n- Computational Creativity\n- Film & Media Technology\n\nIt focuses on AI-assisted creative systems rather than predictive modeling, emphasizing design logic, emotional representation, and responsible automation.\n",
      "metadata": {}
    },
    {
      "id": "be0e5d62-ed72-4a3a-9d42-39f36f46015c",
      "cell_type": "markdown",
      "source": "## Real-world Relevance: \nThis tool allows creators to rapidly prototype emotional subtexts, using heuristic AI to automate complex editorial structures like \"Anxious\" pulsing or \"Flashback\" erasures.\n\nEmotion-driven editing is widely used in:\n- Film trailers\n- Mental health storytelling\n- Artistic short films\n- Social impact media\n\nIndependent creators often lack access to advanced post-production tools. This system demonstrates how lightweight AI logic can help creators express complex emotional states programmatically, using accessible libraries and transparent logic.\n",
      "metadata": {}
    },
    {
      "id": "bb670a59-c3de-48f1-bd41-1e7c6ca8b9e3",
      "cell_type": "markdown",
      "source": "## Data Understanding & Preparation\n\n### Input Data\n- Three short video clips representing narrative fragments\n- An user-selected emotional state (`mood`) either by choosing from a dropdown list or manual commands.\n\n### Assumptions\n- All clips share compatible resolution or are composited\n- Emotion is represented symbolically via editing rhythm, pacing, and repetition\n- No facial recognition or biometric data is used",
      "metadata": {}
    },
    {
      "id": "31eaac8a-4feb-4588-a95e-23991b21b8ea",
      "cell_type": "markdown",
      "source": "## Libraries Used\n\n- Python 3.x\n- MoviePy\n- NumPy\n- FFmpeg (backend)\n\nThis notebook assumes all dependencies are installed locally.",
      "metadata": {}
    },
    {
      "id": "5f97863f-2cee-4313-9126-373427ccefb7",
      "cell_type": "markdown",
      "source": "## Imports & Setup",
      "metadata": {}
    },
    {
      "id": "1cc344af-facb-45d3-8114-a3436c0ae9d3",
      "cell_type": "code",
      "source": "import numpy as np\nimport re\n\nfrom moviepy.editor import (\n    VideoFileClip,\n    concatenate_videoclips,\n    ColorClip\n)\n\nimport moviepy.video.fx.all as vfx\nfrom scipy.ndimage import gaussian_filter\nfrom IPython.display import Video",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "7fcdd9cc-e4d2-4876-a683-04ef88a5b0e7",
      "cell_type": "markdown",
      "source": "## Model / System Design",
      "metadata": {}
    },
    {
      "id": "8f6e1b0a-de65-4b26-a605-240ef94bbda9",
      "cell_type": "markdown",
      "source": "This system follows a **rule-based emotional montage architecture**:\n\n- **Anxious** â†’ Rapid interruptions with black frames (visual pulse)\n- **Flashback** â†’ Forward playback followed by accelerated reverse\n- **Neutral** â†’ Smooth linear continuity\n\nThe \"model\" here is not a neural network but a **deterministic creative system**, where cinematic rules encode emotional intent.\n",
      "metadata": {}
    },
    {
      "id": "80f7cfa3-4b08-42c6-9021-7361783a99a9",
      "cell_type": "markdown",
      "source": "## Logic ",
      "metadata": {}
    },
    {
      "id": "36213b62-9d6c-4caf-a975-4567a86b960c",
      "cell_type": "code",
      "source": "# ---------------- PAGE SETUP ----------------\nst.set_page_config(page_title=\"Lev AI | Meaning Between Cuts\", layout=\"wide\")\nst.title(\"ðŸŽ¬ Meaning Between Cuts\")\nst.subheader(\"AI Montage Tool by Deepikha Bharadwaj\")\n\n# ---------------- INPUT SECTION ----------------\nc1, c2, c3 = st.columns(3)\nwith c1: f1 = st.file_uploader(\"Neutral Shot\", type=[\"mp4\", \"mov\"])\nwith c2: f2 = st.file_uploader(\"Context Shot 1\", type=[\"mp4\", \"mov\"])\nwith c3: f3 = st.file_uploader(\"Context Shot 2\", type=[\"mp4\", \"mov\"])\n\nmood = st.selectbox(\"Narrative Mood\", [\"Peaceful\", \"Flashback\", \"Anxious\", \"Haze\", \"Noir\", \"None\"])\n\nmanual_enabled = (mood == \"None\")\nif manual_enabled:\n    p_in = st.text_input(\"Manual Effects (e.g., zoom 10, bw all, grain second, reverse)\")\nelse:\n    st.info(f\"Active Mode: {mood}\")\n    p_in = \"\"\n\n# ---------------- ROBUST EFFECT FUNCTIONS ----------------\ndef grayscale_math(get_frame, t):\n    frame = get_frame(t)\n    gray = np.dot(frame[..., :3], [0.299, 0.587, 0.114])\n    return np.stack([gray]*3, axis=-1).astype(\"uint8\")\n\ndef haze_math(get_frame, t):\n    frame = get_frame(t)\n    blurred = np.zeros_like(frame)\n    for i in range(3):\n        blurred[:, :, i] = gaussian_filter(frame[:, :, i], sigma=8)\n    return blurred.astype(\"uint8\")\n\n# ---------------- MAIN PIPELINE ----------------\nif st.button(\"GENERATE CINEMATIC MONTAGE\"):\n    if not (f1 and f2 and f3):\n        st.error(\"Please upload all three shots.\")\n    else:\n        with st.spinner(f\"Processing {mood} Montage...\"):\n            try:\n                # 1. Clean up old files to prevent conflicts\n                for old_file in [\"out.mp4\", \"a.mp4\", \"b.mp4\", \"c.mp4\"]:\n                    if os.path.exists(old_file): os.remove(old_file)\n\n                # 2. Save new uploads\n                for name, f in zip([\"a\", \"b\", \"c\"], [f1, f2, f3]):\n                    with open(f\"{name}.mp4\", \"wb\") as out: out.write(f.getbuffer())\n\n                # 3. Load & Process Clips\n                v1 = VideoFileClip(\"a.mp4\").resized(height=1080)\n                v2 = VideoFileClip(\"b.mp4\").resized(height=1080)\n                v3 = VideoFileClip(\"c.mp4\").resized(height=1080)\n\n                def process(clip, tag):\n                    if mood == \"Noir\": clip = clip.transform(grayscale_math)\n                    if mood == \"Haze\": clip = clip.transform(haze_math)\n                    if manual_enabled and p_in:\n                        p = p_in.lower()\n                        if tag in p or \"all\" in p:\n                            if \"bw\" in p: clip = clip.transform(grayscale_math)\n                            if \"reverse\" in p: clip = clip.with_effects([vfx.TimeMirror()])\n                            if \"zoom\" in p:\n                                match = re.search(r\"zoom (\\d+)\", p)\n                                z = float(match.group(1))/100 if match else 0.1\n                                clip = clip.resized(lambda t: 1 + z*t)\n                    return clip\n\n                s1, s2, s3 = process(v1, \"first\"), process(v2, \"second\"), process(v3, \"third\")\n\n                # 4. ASSEMBLY LOGIC FOR ALL MODES\n                if mood == \"Anxious\":\n                    black = ColorClip(size=v1.size, color=(0,0,0), duration=0.25)\n                    content = concatenate_videoclips([s1, black, s2, black, s3], method=\"compose\")\n                elif mood == \"Flashback\":\n                    fwd = concatenate_videoclips([s1, s2, s3], method=\"compose\")\n                    rev = fwd.with_effects([vfx.TimeMirror(), vfx.MultiplySpeed(4.0)])\n                    content = concatenate_videoclips([fwd, rev], method=\"compose\")\n                else:\n                    # Peaceful, Haze, Noir, and None all use smooth transitions\n                    content = concatenate_videoclips([s1, s2, s3], method=\"compose\", padding=-0.5)\n\n                # 5. SECURE EXPORT\n                # Explicitly naming a new file each time helps avoid browser cache issues\n                output_name = f\"montage_{int(time.time())}.mp4\"\n                content.write_videofile(output_name, codec=\"libx264\", audio_codec=\"aac\", fps=24, logger=None)\n\n                # 6. FORCE RELEASE FILE\n                v1.close(); v2.close(); v3.close(); content.close()\n                time.sleep(1) # Short pause to ensure the OS releases the file\n\n                # 7. DISPLAY & DOWNLOAD\n                st.video(output_name)\n                with open(output_name, \"rb\") as f:\n                    st.download_button(\"ðŸ“¥ Download Montage\", f, file_name=\"my_cinematic_edit.mp4\")\n                st.success(f\"{mood} sequence generated successfully!\")\n\n            except Exception as e:\n                st.error(f\"Render Error: {e}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'cinematic_montage.mp4'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "id": "c1e77884-2edb-4870-bf3d-979bdf232092",
      "cell_type": "markdown",
      "source": "## Evaluation & Analysis\n\n### Qualitative Evaluation\n- Emotional intent is communicated through pacing and repetition\n- Anxiety is reinforced via visual interruption\n- Flashbacks simulate memory distortion\n\n### Limitations\n- Emotion selection is manual\n- No learning or personalization\n- Limited clip diversity\n\nDespite this, the system succeeds as a proof-of-concept for emotionally aware creative automation.\n",
      "metadata": {}
    },
    {
      "id": "95102c0f-0c41-44d7-8f03-12985fd5f062",
      "cell_type": "markdown",
      "source": "## Ethical Considerations & Responsible AI\n\n- No personal, biometric, or sensitive data is used\n- The system avoids emotion inference or psychological diagnosis\n- Emotions are artist-defined, not user-profiled\n- Creative control remains fully with the human creator\n\nThis aligns with Responsible AI principles of transparency, agency, and non-exploitation.\n",
      "metadata": {}
    },
    {
      "id": "184ddf69-e762-4f36-8677-265512f48a01",
      "cell_type": "markdown",
      "source": "## Conclusion & Future Scope\n\nThis project demonstrates how AI-assisted systems can support emotional storytelling without replacing human creativity.\n\n### Future Enhancements\n- Emotion detection from audio/music\n- ML-based pacing optimization\n- Integration with mobile filmmaking tools\n- Expanded emotional vocabulary\n\nThe system positions AI as a creative collaborator, not an author.\n",
      "metadata": {}
    }
  ]
}